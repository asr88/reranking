{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Query Rewriting en sistemas RAG\n",
    "\n",
    "En un sistema RAG, la calidad de la recuperación depende no solo del índice vectorial y de los embeddings, sino también de **cómo está formulada la consulta**. Consultas ambiguas, largas o con vocabulario poco alineado al corpus pueden provocar recuperación de fragmentos irrelevantes.\n",
    "\n",
    "El **query rewriting** reformula la consulta original *antes* de la recuperación, usando un modelo de lenguaje para generar una o varias versiones optimizadas para búsqueda semántica. Se aplica justo antes del retriever, sin modificar el índice ni los embeddings.\n",
    "\n",
    "En este notebook se implementan: **zero-shot**, **few-shot**, **sub-queries**, **step-back** y **HyDE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "language_model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot Query Rewriting\n",
    "\n",
    "El modelo recibe la consulta y genera **múltiples reformulaciones** sin ejemplos previos, actuando como generador de sinónimos y variaciones que preservan la intención."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_rewrite_prompt = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query.\n",
    "\n",
    "Perform query expansion. If there are multiple common ways of phrasing a user query\n",
    "or common synonyms for key words in the query, make sure to return multiple versions\n",
    "of the query with the different phrasings.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\n",
    "\n",
    "Return exactly 3 different rewritten versions of the query.\n",
    "Do not include explanations, commentary, or any other text besides the numbered rewritten queries.\"\"\"\n",
    "\n",
    "zero_shot_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_rewrite_prompt),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = zero_shot_prompt | language_model\n",
    "response = chain.invoke({\"question\": \"Which food items does this recipe need?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot Query Rewriting\n",
    "\n",
    "Se incorporan **ejemplos** (pregunta → reformulación) que guían al modelo para un estilo más consistente y conciso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_examples = [\n",
    "    {\n",
    "        \"question\": \"How tall is the Eiffel Tower? It looked so high when I was there last year\",\n",
    "        \"answer\": \"What is the height of the Eiffel Tower?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"1 oz is 28 grams, how many cm is 1 inch?\",\n",
    "        \"answer\": \"Convert 1 inch to cm.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What's the main point of the article? What did the author try to convey?\",\n",
    "        \"answer\": \"What is the main key point of this article?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{question}\"),\n",
    "    (\"ai\", \"{answer}\"),\n",
    "])\n",
    "\n",
    "few_shots = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=few_shot_examples\n",
    ")\n",
    "\n",
    "few_shot_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_rewrite_prompt),\n",
    "    few_shots,\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chain = few_shot_prompt | language_model\n",
    "response = chain.invoke({\"question\": \"Which food items does this recipe need?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-queries\n",
    "\n",
    "Se **descompone** una consulta compleja en varias preguntas más simples. Útil cuando la pregunta incluye múltiples aspectos o condiciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_decompose_prompt = \"\"\"You are a helpful assistant that generates search queries based on a single input query.\n",
    "\n",
    "Perform query decomposition. Given a user query, break it down into distinct sub-queries that\n",
    "must be answered in order to answer the original query.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\n",
    "\n",
    "Return only the decomposed sub-queries.\n",
    "Do not include explanations, commentary, or any other text besides the numbered sub-queries.\"\"\"\n",
    "\n",
    "subqueries_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_decompose_prompt),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chain = subqueries_prompt | language_model\n",
    "response = chain.invoke({\n",
    "    \"question\": \"Which is the most popular programming language for machine learning and is it the most popular programming language overall?\"\n",
    "})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-back\n",
    "\n",
    "Se reformula la pregunta **específica** en una versión más **general** que capture los principios necesarios para responderla. Útil cuando la consulta es muy restrictiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_step_back_prompt = \"\"\"You are an expert at taking a specific query and extracting a more generic query that captures\n",
    "the underlying principles needed to answer the specific query.\n",
    "\n",
    "Given a specific user query, write a more generic query that must be answered in order to answer the specific query.\n",
    "\n",
    "If you don't recognize a word or acronym, do not try to rewrite it.\n",
    "\n",
    "Write a concise generic query.\n",
    "Return only the rewritten generic query.\n",
    "Do not include explanations, commentary, or any other text.\"\"\"\n",
    "\n",
    "step_back_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_step_back_prompt),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chain = step_back_prompt | language_model\n",
    "response = chain.invoke({\n",
    "    \"question\": \"Which is the most popular programming language for machine learning?\"\n",
    "})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyDE (Hypothetical Document Embeddings)\n",
    "\n",
    "Las preguntas y las respuestas no siempre son semánticamente similares. **HyDE** genera un **documento hipotético** (respuesta tentativa) y usa su embedding para recuperar, ya que respuestas similares suelen estar más cerca en el espacio vectorial que pregunta-respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "actual_document = \"\"\"\n",
    "Berkson's paradox describes a form of selection bias in which observing a relationship between two variables within a selected\n",
    "subpopulation can create a misleading statistical association that does not exist—or is reversed—in the overall population.\n",
    "\n",
    "This paradox typically arises when inclusion in the observed sample depends on both variables being studied. Because the\n",
    "selection criterion is influenced by the variables themselves, conditioning on that criterion distorts their apparent relationship.\n",
    "\"\"\"\n",
    "\n",
    "actual_document_embedding = embeddings_model.embed_documents([actual_document])\n",
    "\n",
    "system_hyde_prompt = \"\"\"You are an expert at using a query to generate a document useful for answering the query.\n",
    "\n",
    "Given a query, generate a paragraph of text that answers the query.\n",
    "\"\"\"\n",
    "\n",
    "hyde_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_hyde_prompt),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chain = hyde_prompt | language_model\n",
    "hypothetical_document = chain.invoke({\"question\": \"What does Berkson's paradox consist on?\"})\n",
    "hypothetical_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embedding = embeddings_model.embed_documents([\"What does Berkson's paradox consist on?\"])\n",
    "hypothetical_document_embedding = embeddings_model.embed_documents([hypothetical_document.content])\n",
    "\n",
    "print(f\"Similarity without HyDE: {cosine_similarity(question_embedding, actual_document_embedding)}\")\n",
    "print(f\"Similarity with HyDE: {cosine_similarity(hypothetical_document_embedding, actual_document_embedding)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
