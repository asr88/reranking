{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Integración de query rewriting y reranking en el flujo RAG\n",
    "\n",
    "Se integran **query rewriting** y **reranking** en un flujo completo usando **LangGraph**: el pipeline se modela como un grafo de estados con etapas claras (reescritura → recuperación → reranking → generación). Cada nodo transforma un estado compartido (**RAGState**).\n",
    "\n",
    "⚠️ **Requisito**: Ejecute antes los notebooks `03_rag_base.ipynb` y `05_reranking.ipynb` (o al menos 03) para tener la carpeta `faiss_index`. Este notebook carga índice, retriever, modelo de lenguaje, prompt de reescritura y cross-encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from sentence_transformers import CrossEncoder\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict, List\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "language_model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "vectorstore = FAISS.load_local(\"./faiss_index\", embeddings_model, allow_dangerous_deserialization=True)\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "\n",
    "system_rewrite_prompt = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query.\n",
    "\n",
    "Perform query expansion. If there are multiple common ways of phrasing a user query\n",
    "or common synonyms for key words in the query, make sure to return multiple versions\n",
    "of the query with the different phrasings.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\n",
    "\n",
    "Return exactly 3 different rewritten versions of the query.\n",
    "Do not include explanations, commentary, or any other text besides the numbered rewritten queries.\"\"\"\n",
    "\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "print(\"Modelos y retriever listos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estado compartido (RAGState)\n",
    "\n",
    "Contrato de datos entre nodos: consulta original, consultas reescritas, documentos (texto), fuentes y respuesta final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGState(TypedDict):\n",
    "    \"\"\"Estado compartido del flujo RAG en LangGraph.\"\"\"\n",
    "    query: str\n",
    "    rewrited_queries: List[str]\n",
    "    documents: List[str]\n",
    "    sources: List[str]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodo: Query rewriting\n",
    "\n",
    "Zero-shot: la consulta original se transforma en varias reformulaciones; la salida se parsea por líneas (eliminando numeración) para obtener la lista de subconsultas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(state: RAGState) -> RAGState:\n",
    "    zero_shot_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_rewrite_prompt),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])\n",
    "    chain = zero_shot_prompt | language_model\n",
    "    response = chain.invoke({\"question\": state[\"query\"]})\n",
    "\n",
    "    lines = response.content.split(\"\\n\")\n",
    "    rewrited_queries = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            content = line[3:].strip() if len(line) >= 3 else line\n",
    "            if content:\n",
    "                rewrited_queries.append(content)\n",
    "\n",
    "    return {\n",
    "        \"query\": state[\"query\"],\n",
    "        \"rewrited_queries\": rewrited_queries,\n",
    "        \"documents\": [],\n",
    "        \"sources\": [],\n",
    "        \"answer\": \"\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodo: Recuperación y deduplicación\n",
    "\n",
    "Se ejecuta el retriever para cada consulta reescrita; se consolidan documentos únicos por contenido y se guardan fuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(state: RAGState) -> RAGState:\n",
    "    query = state[\"query\"]\n",
    "    rewrited_queries = state[\"rewrited_queries\"]\n",
    "    seen_contents = set()\n",
    "    documents = []\n",
    "    sources = []\n",
    "\n",
    "    for subquery in rewrited_queries:\n",
    "        docs = retriever.invoke(subquery)\n",
    "        for doc in docs:\n",
    "            content = doc.page_content\n",
    "            if content not in seen_contents:\n",
    "                seen_contents.add(content)\n",
    "                documents.append(content)\n",
    "                sources.append(doc.metadata.get(\"source_file\", \"unknown\"))\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"rewrited_queries\": rewrited_queries,\n",
    "        \"documents\": documents,\n",
    "        \"sources\": sources,\n",
    "        \"answer\": \"\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodo: Reranking\n",
    "\n",
    "Se evalúa cada par (consulta **original**, documento) con el cross-encoder, se ordena por puntaje y se conservan los top-k (p. ej. 3) documentos y sus fuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_documents(state: RAGState) -> RAGState:\n",
    "    documents = state[\"documents\"]\n",
    "    sources = state[\"sources\"]\n",
    "    query = state[\"query\"]\n",
    "\n",
    "    pairs = [(query, doc) for doc in documents]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    scored_docs = list(zip(documents, sources, scores))\n",
    "    reranked = sorted(scored_docs, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    top_k = 3\n",
    "    reranked_docs = [doc for doc, _, _ in reranked[:top_k]]\n",
    "    reranked_sources = [src for _, src, _ in reranked[:top_k]]\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"rewrited_queries\": state[\"rewrited_queries\"],\n",
    "        \"documents\": reranked_docs,\n",
    "        \"sources\": reranked_sources,\n",
    "        \"answer\": \"\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodo: Generación de la respuesta\n",
    "\n",
    "Se construye el contexto con los documentos rerankeados y se invoca el modelo con un prompt que restringe la respuesta al contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Eres un asistente especializado en responder preguntas sobre documentación técnica de desarrollo\n",
    "de software. Utiliza únicamente la información del contexto proporcionado para responder la pregunta.\n",
    "Si no conoces la respuesta basándote en el contexto, indica claramente que no tienes esa información.\n",
    "\n",
    "Pregunta: {question}\n",
    "\n",
    "Contexto: {context}\n",
    "\n",
    "Respuesta:\n",
    "\"\"\")\n",
    "\n",
    "def generate_answer(state: RAGState) -> RAGState:\n",
    "    context = \"\\n\\n\".join(state[\"documents\"])\n",
    "    chain = prompt | language_model\n",
    "    response = chain.invoke({\n",
    "        \"context\": context,\n",
    "        \"question\": state[\"query\"]\n",
    "    })\n",
    "    return {\n",
    "        **state,\n",
    "        \"answer\": response.content\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grafo LangGraph y compilación\n",
    "\n",
    "Secuencia: rewrite_query → retrieve_documents → rerank_documents → generate_answer → END."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(RAGState)\n",
    "graph.add_node(\"rewrite_query\", rewrite_query)\n",
    "graph.add_node(\"retrieve_documents\", retrieve_documents)\n",
    "graph.add_node(\"rerank_documents\", rerank_documents)\n",
    "graph.add_node(\"generate_answer\", generate_answer)\n",
    "\n",
    "graph.set_entry_point(\"rewrite_query\")\n",
    "graph.add_edge(\"rewrite_query\", \"retrieve_documents\")\n",
    "graph.add_edge(\"retrieve_documents\", \"rerank_documents\")\n",
    "graph.add_edge(\"rerank_documents\", \"generate_answer\")\n",
    "graph.add_edge(\"generate_answer\", END)\n",
    "\n",
    "rag_app = graph.compile()\n",
    "print(\"Grafo RAG compilado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfaz ask_rag y demostración\n",
    "\n",
    "Inicializa el estado, ejecuta el grafo y devuelve el estado final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_rag(question: str) -> RAGState:\n",
    "    initial_state: RAGState = {\n",
    "        \"query\": question,\n",
    "        \"rewrited_queries\": [],\n",
    "        \"documents\": [],\n",
    "        \"sources\": [],\n",
    "        \"answer\": \"\"\n",
    "    }\n",
    "    return rag_app.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ask_rag(\"¿Qué son las APIs REST y cuáles son sus principios fundamentales?\")\n",
    "\n",
    "print(\"\\nRESPUESTA GENERADA POR EL SISTEMA RAG\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nPregunta original:\\n{result['query']}\")\n",
    "print(\"\\nConsultas reescritas:\" + \"-\" * 80)\n",
    "for i, q in enumerate(result[\"rewrited_queries\"], 1):\n",
    "    print(f\"{i}. {q}\")\n",
    "print(\"\\nRespuesta:\" + \"-\" * 80)\n",
    "print(result[\"answer\"])\n",
    "print(\"\\nFuentes utilizadas:\" + \"-\" * 80)\n",
    "for i, source in enumerate(result[\"sources\"], 1):\n",
    "    print(f\"{i}. {source}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
